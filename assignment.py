# -*- coding: utf-8 -*-
"""assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w-9HIQIfqvxfyPlMN9eRq4c3kcOh2KTb
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

# Read the URLs from the input Excel file
input_df = pd.read_excel('Input.xlsx')

# Function to extract article text from a given URL
def extract_article_text(url):
    try:
        # Send a GET request to the URL
        response = requests.get(url)
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.content, 'html.parser')
        # Find the article title
        title = soup.find('title').get_text()
        # Find the article text
        article_text = ""
        # Assuming the article text is contained within <p> tags
        paragraphs = soup.find_all('p')
        for paragraph in paragraphs:
            article_text += paragraph.get_text() + "\n"

        return title, article_text
    except Exception as e:
        print(f"Error occurred while extracting text from {url}: {e}")
        return None, None

# Iterate over each row in the input DataFrame
for index, row in input_df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']

    # Extract article text from the URL
    title, article_text = extract_article_text(url)

    # Save the extracted article text into a text file
    if article_text:
        filename = f"{url_id}.txt"
        with open(filename, 'w', encoding='utf-8') as file:
            file.write(title + "\n\n")
            file.write(article_text)

print("Data extraction completed.")

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from textblob import TextBlob
import pandas as pd

# Load NLTK resources
nltk.download('punkt')

# Define positive and negative dictionaries
positive_dictionary = {
    'good': 1,
    'excellent': 1,
    'happy': 1,
    # Add more positive words as needed
}

negative_dictionary = {
    'bad': 1,
    'terrible': 1,
    'unhappy': 1,
    # Add more negative words as needed
}

# Function to calculate positive and negative scores
def calculate_pos_neg_scores(article_text):
    # Tokenize text into words
    words = word_tokenize(article_text)

    # Initialize positive and negative scores
    positive_score = 0
    negative_score = 0

    # Calculate positive score
    for word in words:
        if word.lower() in positive_dictionary:
            positive_score += positive_dictionary[word.lower()]

    # Calculate negative score
    for word in words:
        if word.lower() in negative_dictionary:
            negative_score += negative_dictionary[word.lower()]

    return positive_score, negative_score

# Update the analyze_text function to include positive and negative scores
def analyze_text(article_text):
    # Tokenize text into sentences and words
    sentences = sent_tokenize(article_text)
    words = word_tokenize(article_text)

    # Compute word count
    word_count = len(words)

    # Compute average sentence length
    avg_sentence_length = sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences)

    # Compute average word length
    avg_word_length = sum(len(word) for word in words) / len(words)

    # Count personal pronouns (he, she, I, etc.)
    personal_pronouns = sum(1 for word, pos in nltk.pos_tag(words) if pos == 'PRP')

    # Count complex words (words with more than 3 syllables)
    complex_words = [word for word in words if len(word) > 6]
    complex_word_count = len(complex_words)

    # Compute percentage of complex words
    percentage_complex_words = (complex_word_count / word_count) * 100 if word_count > 0 else 0

    # Compute FOG Index
    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)

    # Compute syllable per word
    syllable_per_word = sum(len(word) for word in words) / len(words) if word_count > 0 else 0

    avg_words_per_sentence = word_count / len(sentences)

    # Use TextBlob to compute polarity and subjectivity scores
    blob = TextBlob(article_text)
    polarity_score = blob.sentiment.polarity
    subjectivity_score = blob.sentiment.subjectivity

    # Calculate positive and negative scores
    positive_score, negative_score = calculate_pos_neg_scores(article_text)

    # Return computed variables
    return {
        'WORD COUNT': word_count,
        'AVG SENTENCE LENGTH': avg_sentence_length,
        'AVG NUMBER OF  WORDS PER SENTENCE ': avg_words_per_sentence,
        'AVG WORD LENGTH':avg_word_length,
        'PERSONAL PRONOUNS': personal_pronouns,
        'COMPLEX WORD COUNT': complex_word_count,
        'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,
        'FOG INDEX': fog_index,
        'SYLLABLE PER WORD': syllable_per_word,
        'POLARITY SCORE': polarity_score,
        'SUBJECTIVITY SCORE': subjectivity_score,
        'POSITIVE SCORE': positive_score,
        'NEGATIVE SCORE': negative_score
    }

# Load the DataFrame containing the URLs and filenames
input_df = pd.read_excel('Input.xlsx')

# Iterate over each row in the input DataFrame
output_data = []
for index, row in input_df.iterrows():
    url_id = row['URL_ID']
    filename = f"{url_id}.txt"

    # Read the content of the text file
    with open(filename, 'r', encoding='utf-8') as file:
        article_text = file.read()

    # Perform text analysis
    analysis_result = analyze_text(article_text)

    # Add URL_ID and analysis results to output data
    analysis_result['URL_ID'] = url_id
    output_data.append(analysis_result)

# Convert output data to DataFrame
output_df = pd.DataFrame(output_data)

# Reorder columns to match the output structure
output_df = output_df[['URL_ID', 'POSITIVE SCORE', 'NEGATIVE SCORE','POLARITY SCORE',
                       'SUBJECTIVITY SCORE','AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS',
                       'FOG INDEX','AVG NUMBER OF  WORDS PER SENTENCE ', 'COMPLEX WORD COUNT','WORD COUNT',
                       'SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH'
                     ]]

# Save output DataFrame to Excel file
output_df.to_excel('Output.xlsx', index=False)

print("Text analysis completed. Output saved to 'Output.xlsx'")

df=pd.read_excel('Output.xlsx')
df